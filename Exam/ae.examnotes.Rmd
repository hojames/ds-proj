---
title: "finalnotes"
author: "alex"
date: "12/5/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

load these
```{r}
library(dslabs)
library(tidyverse)
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(Lahman)
library(caret)
library(broom)
library(lubridate)
library(stringr)
library(XML)
library(gridExtra)
library(HistData)
library(reshape2)
library(lpSolve)
library(ggrepel)
library(rvest)
library(purrr)
```


Spearman correlation --> confounding.Rmd
IF OUTLIERS IN DATA: Spearman Correlation: An alternative to the sample correlation for estimating the population correlation that is robust to outliers. The idea is simple: compute the correlation on the ranks of the values. 
```{r}
cor(rank(x), rank(y))
#or this code below
cor(x, y, method = "spearman")
```

Confounding --> confounding.Rmd
If $X$ and $Y$ are correlated, we call $Z$ a _confounder_ if changes in $Z$ cause changes in both $X$ and $Y$. 
```{r}
data(admissions)
admissions

#The percent of men and women that were accepted was:
admissions %>% group_by(gender) %>% 
  summarize(percentage = 
              round(sum(admitted*applicants)/sum(applicants),1))

# A statistical test clearly rejects the hypothesis that gender and admission are independent:
admissions %>% group_by(gender) %>% 
  summarize(total_admitted = round(sum(admitted/100*applicants)), 
            not_admitted = sum(applicants) - sum(total_admitted)) %>%
  select(-gender) %>% 
  do(tidy(chisq.test(.)))

# But closer inspection shows a paradoxical result. Here are the percent admissions by major:
admissions %>% select(major, gender, admitted) %>%
  spread(gender, admitted) %>%
  mutate(women_minus_men = women - men)
#seems to favor women slightly... 

#graphically
admissions %>% 
  ggplot(aes(major, admitted, col = gender, size = applicants)) +
  geom_point()
```


# Sampling --> intro-to-regression.Rmd
In most data science applications, we do not observe the population but rather a sample. As with the average and standard deviation, the **sample** correlation is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.
```{r}
#example
R <- sample_n(galton_heights, 25, replace = TRUE) %>% 
  summarize(cor(father, son))
#poor competitor of galton - this is just a random sample of 25
#this corr is a random variable so it will change each time you do it

#the above is a random variable. We can run a Monte Carlo simulation to see its distribution:
B <- 1000
N <- 25
R <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>% 
    summarize(r=cor(father, son)) %>% .$r
})
data.frame(R) %>% ggplot(aes(R)) + geom_histogram(binwidth = 0.05, color = "black")
```
Will have larger se usually than full data set (since you are sampling a smaller data)

Also note that because the sample correlation is an average of independent draws, the central limit theorem actually applies. Therefore, for large enough $N$, the distribution of `R` is approximately normal with expected value $\rho$. The standard deviation is somewhat complex to derive: it is $\sqrt{\frac{1-r^2}{N-2}}$. 


In our example, $N=25$ does not seem to be large enough to make the approximation a good one:
```{r}
data.frame(R) %>% 
  ggplot(aes(sample=R)) + 
  stat_qq() + 
  geom_abline(intercept = mean(R), 
              slope = sqrt((1-mean(R)^2)/(N-2)))
#doesnt quite look normal, but if you change N to 500, it'll defintiely fall on the line
```


Conditional avg --> intro-to-regression.Rmd
what if we are told that the father is 72 inches tall, do we sill guess `r mean(galton_heights$son)` for the son? we need to round because there are not that many observations for exaclty 72 inches tall
```{r}
conditional_avg <- galton_heights %>% 
  filter(round(father) == 72) %>%
  summarize(avg = mean(son)) %>% .$avg
conditional_avg
```


adding regression line into ggplot --> linear-models.Rmd
```{r}
geom_smooth(method = "lm")
```

computing linear regression model --> linear-models.Rmd
```{r}
fit <- lm(y ~ x, data = df)
summary(fit)
```

