---
title: "R Notebook"
output: html_notebook
---
##PCA

PCA is related to a matrix factorization technique called the _Singular Value Decomposition_ which is based on a result that tells us that any matrix $\mathbf{Y}$ can be factorized into two matrices.

If we run PCA on the $r$ matrix, the first PC will be the vector $p_{1,1}, \dots p_{M,1}$ that minimizes the squared error of the approximation. 

$$
r_{u,i} \approx p_{u,1} q_{1,i} + p_{u,2} q_{2,i}
$$

Using PCA on netflix:
```{r}
r[is.na(r)] <- 0
pca <- prcomp(r - rowMeans(r), center=TRUE, scale=FALSE) 

library(ggrepel)

tmp <- data.frame(pca$rotation, name = movie_titles) ##this would return the loadings for each variable and every principal component
```


##KNN

Using k-nearest neighbors to create meaningful groups/distinctions in data. 

"K-nearest neighbors (kNN) is similar to bin smoothing, but it is easier to adapt to multiple dimensions. We first define the distance between all observations based on the features.Basically, for any point xx for which we want an estimate of p(x)p(x), we look for the kk nearest points and then take an average of these points. This gives us an estimate of p(x1,x2)p(x1,x2), just like the bin smoother gave us an estimate of a curve. We can now control flexibility through kk.""

```{r}
knn_fit_251 <- knn3(y~.,data = select(train_set, y, X_1, X_2), k=5) # use knn on training data, default k is 5
f_hat <- predict(knn_fit_5, newdata = test_set)[,2] #create test set data using knn
tab <- table(pred=round(f_hat), truth=test_set$y) #compare test and predict
confusionMatrix(tab)$overall["Accuracy"]
```

To avoid overtraining (too many islands when it should be a smooth curve, small k) or oversmooething (uninformative line splitting the data, big k), we can plot the accuracy against number of k. Pick the k that approximates best accuracy. 

```{r}
control <- trainControl(method='cv', number=2, p=.5)
dat2 <- mutate(dat, label=as.factor(label)) %>%
  select(label,X_1,X_2)
res <- train(label ~ .,
             data = dat2,
             method = "knn",
             trControl = control,
             tuneLength = 1, # How fine a mesh to go on grid
             tuneGrid=data.frame(k=seq(3,151,2)),
             metric="Accuracy")
plot(res)
```

##Euclidean Distance

Remember Euclidean distance is the distance between two points, and you could use the pythagorean theorem to figure this out on an XY plane. With high dimensional data, we no longer have a 2-dimensional XY plane. But the distance is similarly computed. For instance, the distance between two observations, say observations $i=1$ and $i=2$ is:

$$
\mbox{dist}(1,2) = \sqrt{ \sum_{j=1}^{784} (Y_{1,j}-Y_{2,j })^2 }
$$ 